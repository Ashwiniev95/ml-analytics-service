[COMMON]

diksha_survey_app_name = <SURVEY_APP_NAME>

diksha_integrated_app_name = <INTEGRATED_APP_NAME>

[API_HEADERS]

content_type = application/json

authorization = <AUTHORIZATION_KEY>

internal_access_token = <ACCESS_TOKEN>

[URL]

base_url = http://<INTERNAL_IP>:<PORT_NUMBER>

url_entity_related = assessment/api/v1/entities/relatedEntities/

url_user_profile_api = assessment/api/v1/userExtension/getProfile/

evidence_base_url = <EVIDENCE_BASE_URL>

[MONGO]

# --------------
# Mongo prod url
#---------------

mongo_url = mongodb://<INTERNAL_IP>:<PORT_NUMBER>

# -----------------------
# Mongo database name
# -----------------------

database_name = <DATABASE_NAME>

# -------------------
# Mongo Collections
# -------------------

observation_sub_collec = <OBSERVATION_SUB_COLLECTION_NAME>

solutions_collec = <SOLUTION_COLLECTION_NAME>

observations_collec = <OBSERVATION_COLLECTION_NAME>

entity_type_collec = <ENTITY_TYPE_COLLECTION_NAME>

questions_collec = <QUESTION_COLLECTION_NAME>

criteria_collec = <CRITERIA_COLLECTION_NAME>

entities_collec = <ENTITIES_COLLECTION_NAME>

programs_collec = <PROGRAM_COLLECTION_NAME>

user_roles_collection = <USERROLES_COLLECTION_NAME>

criteria_questions_collection = <CRITERIA_QUESRIONS_COLLECTION_NAME>

projects_collection = <PROJECTS_COLLECTION_NAME>

survey_submissions_collection = <SURVEY_SUBMISSIONS_COLLECTION>

survey_collection = <SURVEY_COLLECTION>

[DRUID]

druid_end_point = http://<INTERNAL_IP>:<PORT_NUMBER>/druid/coordinator/v1/datasources/

druid_batch_end_point = http://<INTERNAL_IP>:<PORT_NUMBER>/druid/indexer/v1/task

observation_status_spec = <OBSERVATION_STATUS_SPEC>

general_unnati_spec = <PROJECT_SPEC>

[KAFKA]

kafka_url = <KAFKA_INTERNAL_IP>:<PORT>

kafka_raw_data_topic = <OBS_TOPIC_NAME>

kafka_druid_topic = <OBS_DRUID_TOPIC_NAME>

kafka_evidence_druid_topic =  <OBS_EVIDENCE_TOPIC_NAME>

kafka_evidence_survey_druid_topic = <SURVEY_EVIDENCE_TOPIC_NAME>

kafka_raw_survey_topic = <SURVEY_TOPIC_NAME>

kafka_survey_druid_topic = <SURVEY_DRUID_TOPIC_NAME>

[LOGS]

observation_streaming_success_log_filename = <FOLDER_PATH>/success.log

observation_streaming_error_log_filename = <FOLDER_PATH>/error.log

observation_streaming_evidence_success_log_filename = <FOLDER_PATH>/success.log

observation_streaming_evidence_error_log_filename = <FOLDER_PATH>/error.log

observation_status_success_log_filename = <FOLDER_PATH>/status/success.log

observation_status_error_log_filename = <FOLDER_PATH>/status/error.log

project_success_log_filename = <SUCCESS_LOG_FILE_PATH>

project_error_log_filename = <ERROR_LOG_FILE_PATH>

survey_evidence_streaming_success_log_filename = <EVIDENCE_SUCCESS_LOG_FILE_PATH>

survey_evidence_streaming_error_log_filename = <EVIDENCE_ERROR_LOG_FILE_PATH>

survey_streaming_success_log_filename = <SUCCESS_LOG_FILE_PATH>

survey_streaming_error_log_filename = <ERROR_LOG_FILE_PATH>

[ELASTICSEARCH]

header = {'Content-Type': 'application/json'}

url_user = http://<INTERNAL_IP>:<PORT_NUMBER>/users/_search/?scroll=1m

user_body = {"size": 10000,"query":{"bool":{"must":[{"match":{"_type":"common"}}]}}}

url_user_scroll = http://<INTERNAL_IP>:<PORT_NUMBER>/_search/scroll

url_entity = http://<INTERNAL_IP>:<PORT_NUMBER>/entities/_search/?scroll=1m

[AZURE]

account_name = <ACCOUNT_NAME>

sas_token = <SAS_TOKEN>

container_name = <CONTAINER_NAME>

blob_path = <OBS_BLOB_PATH>

projects_blob_path = <PROJECTS_BLOB_PATH>

[REDIS]

host = <HOST_ADDRESS>

port = <PORT>

db_name = <REDIS_DB_NAME>

[OUTPUT_DIR]

projects_folder = <PROJECTS_OUTPUT_DIR>

observation_status_output_dir = <OBS_STATUS_OUTPUT_DIR>